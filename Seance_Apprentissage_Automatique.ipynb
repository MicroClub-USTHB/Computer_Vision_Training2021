{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seance_Apprentissage_Automatique.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S1wG91RfLQm"
      },
      "source": [
        "**Info**\r\n",
        "\r\n",
        "\r\n",
        "*   **Computer vision training camp** for Micro Club's 1st wave of internal trainings.\r\n",
        "* **Trainer:**  Sarra Laksaci -> SarraLKSC#7509 , saralaksaci@gmail.com\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTJR-nPsE2tY"
      },
      "source": [
        "# **Intro**\r\n",
        "\r\n",
        "Toute au long de la formation on a parlé du Deep Learning. On sait maintenant que le but premier d'un réseaux de neurones c'est d'apprendre sur des données afin d'aboutir à des résultats satisfaisants. D'un point de vu plus technique le but est d'atteindre les poids optimaux qui permettront de minimiser la perte enregister. (On le rappel la perte est la fonction qui permet de juger la distance entre la sortie obtenue et la sortie souhaitée)\r\n",
        "Dans un apprentissage classique ces poids sont tout d'abord choisis aléatoirement, puis au fil de l'apprentissage (au fil des epoques) les poids sont mit à jour jusqu'à aboutir au bonnes valeurs. \r\n",
        "- Le fait de commencer de poids aléatoire rallonge le temps d'apprentissage car ils peuvent etre proches mais aussi loins du but ce qui fait que durant les premiers temps le réseau tatonne un peu dans tous les sens. \r\n",
        "- De plus dans certaines cas (comme la classification d'images) il y a toujours une première partie de l'apprentissage qui est communes à toutes les problématiques (données traitées) ex: apprendre à reconnaitre les cerlce,les droites et toute autre structure basique reconnue pendant le feature extraction.\r\n",
        "- Enfin le scénario dans lequel on se retrouve certaine fois est le suivant : peu de données ... très peu de données (10 images) ... (augmentation can only go so far)\r\n",
        "On peu tomber dans certaines situation ou nous avons trop peu de données pour que le modèle puisse apprendre (ex 20% de taux d'exactitude en 15 epoques, le reflexe sera alors d'augmenter les epoques.. ceci nous permettra peu etre d'atteindre 30% d'exactitude mais encore une fois sur le meme ensemble de données petit donc le modèle pourra overfit mais jamais apprendre)\r\n",
        "\r\n",
        "\r\n",
        "Et puis comme on ne cesse de le répéter le Deep Learning est une méthode gourmande en terme de données. Travailler sur 10 images n'aurait aucun sens en apprentissage classique, L'apprentissage par transfert en revanche nous permet d'accepter des problématiques dont les dataset sont petits.\r\n",
        "\r\n",
        "\r\n",
        ">>>![](https://machinelearningmastery.com/wp-content/uploads/2016/08/Why-Deep-Learning-1024x742.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNymwfknFB1C"
      },
      "source": [
        "# **L'apprentissage par transfert**\r\n",
        "\r\n",
        "##Le concept\r\n",
        "\r\n",
        "L'apprentissage par transfert comme son nom l'indique à pour concept de transferer l'apprentissage d'un modèle vers un autre. Plus précisément on réutilise un modèle ayant été entrainé sur un dataset de grande taille et ayant obtenue d'excellents résultats sur ce dataset (Généralement pour le computer vision on utilise des modèles connus tel que VGG16 ResNet50 qui ont été entrainé sur ImageNet) les connaissances de ce modèle sont par la suite transférer vers le nouveau modèle que nous souhaitons mettre en place pour résoudre une nouvelle problématique dont le dataset est réduit.\r\n",
        "\r\n",
        "Par transfert de connaissances, en Deep Learning, on sous-entend réutiliser les poids optimaux appris par le modèles de base qu'on appelera modèle pré-entrainé.\r\n",
        "\r\n",
        "![](https://miro.medium.com/max/5252/1*Z11P-CjNYWBofEbmGQrptA.png)\r\n",
        "\r\n",
        "En pratique le transfert learning ce fait de deux manières possibles:\r\n",
        "\r\n",
        "- Freeze and use : ici on estime que le problème résolue par le modèle pré-entrainé se rapproche assez du nouveau problème traité pour directement importé l'architecture du modèle pré-entrainé et figer ces poids optimaux pour les utiliser. Par figer on entend que les poids seront considérés comme des costantes lors de l'apprentissage du modèle et ne seront pas mit à jours lors de la décente du gradient. Cela signifie que l'on estime que les poids optimaux du modèle pré-entrainé sont aussi les poids optimaux du nouveau modèle (pour cette partie ex feature extraction) ils sont directement pris tel-quel \r\n",
        "- Finetune : ici on estime que le problème résolu par le modèle pré-entrainé bien que similaire qu nouveau problème contient quand meme quelques différence, on souhaite alors bénificier de l'apprentissage du 1er modèle sans pour autant se limiter complétement en figeant tous ces poids. Ceci signifie que les poids du modèle pré-entrainé seront importés et utilisé dans notre nouveau réseau comme points de départ pour son entrainement \r\n",
        "\r\n",
        "##L'avantage \r\n",
        "\r\n",
        "- résoudre des problèmes avec moins de données \r\n",
        "- résoudres des problèmes en moins de temps \r\n",
        "- résoudre des problèmes avec moins de ressources nécessaires"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4AyZhswFIbb"
      },
      "source": [
        "# **Implémentation avec tensorflow**\r\n",
        "\r\n",
        "##TensorflowHub\r\n",
        "\r\n",
        "Le blessed land of pretrained models [here](https://www.tensorflow.org/hub?hl=fr)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UvMbSXzEz-K"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import tensorflow as tf \r\n",
        "import tensorflow_hub as hub\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import tensorflow_datasets as tfds\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QviQwmN1xLQt"
      },
      "source": [
        "#importer un model de Tensorflow Hub\r\n",
        "URL=\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/4\"\r\n",
        "#l'imbriquer dans une architecture \r\n",
        "model=tf.keras.Sequential([\r\n",
        "                           hub.KerasLayer(URL,trainable=False,input_shape=(224,224,3))\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l10SyiWm0cpO",
        "outputId": "ef79e984-a07e-45ba-cdc6-0be940fc2b10"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_1 (KerasLayer)   (None, 1001)              3540265   \n",
            "=================================================================\n",
            "Total params: 3,540,265\n",
            "Trainable params: 0\n",
            "Non-trainable params: 3,540,265\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJI3qTuy1EjT"
      },
      "source": [
        "**Example d'utilisation de model pré-entrainé** [classification avec mobilnet](hhttps://colab.research.google.com/drive/1hxeKVd4VxoNYsAhfmCNmN8YJWB07hfhI?usp=sharing)\r\n",
        "\r\n",
        "\r\n",
        "**Example de transfer learning sur Pokemon** [avec mobilnet](https://colab.research.google.com/drive/1NhF_tPvqWxh8CHfwTFXk5oPsuv7yVL67?usp=sharing)"
      ]
    }
  ]
}